{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\projects\\\\Delivery-time-prediction-for-food-devlivery-industry\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\projects\\\\Delivery-time-prediction-for-food-devlivery-industry'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformerConfig:\n",
    "    root_dir: Path\n",
    "    data_input_dir: Path\n",
    "    data_tran_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\config\\config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\params.yaml\")\n",
    "SCHEMA_FILE_PATH = Path(\"E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\schema.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deliveryprediction.constants import *\n",
    "from Deliveryprediction.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_trans_config(self) -> DataTransformerConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_Transformation_config = DataTransformerConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_input_dir = config.data_input_dir,\n",
    "            data_tran_dir = config.data_tran_dir\n",
    "        )\n",
    "\n",
    "        return data_Transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, \n",
    "    MinMaxScaler, \n",
    "    OrdinalEncoder)\n",
    "import joblib\n",
    "from sklearn import set_config\n",
    "\n",
    "# set the transformer outputs to pandas\n",
    "set_config(transform_output='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config :DataTransformerConfig):\n",
    "        self.config = config\n",
    "        self.root_path = Path(self.config.data_input_dir)\n",
    "        self.train_data_path = self.root_path / \"train.csv\"\n",
    "        self.test_data_path = self.root_path / \"test.csv\"\n",
    "        self.save_data_dir = Path(self.config.data_tran_dir)\n",
    "        self.save_data_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.train_trans_filename = \"train_trans.csv\"\n",
    "        self.test_trans_filename = \"test_trans.csv\"\n",
    "        self.save_train_trans_path = self.save_data_dir / self.train_trans_filename\n",
    "        self.save_test_trans_path = self.save_data_dir / self.test_trans_filename\n",
    "        self.transformer_save_dir = self.save_data_dir / \"models\"\n",
    "        self.transformer_save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.num_cols = [\"age\", \"ratings\", \"pickup_time_minutes\", \"distance\"]\n",
    "        self.nominal_cat_cols = ['weather', 'type_of_order', 'type_of_vehicle', 'festival',\n",
    "                                 'city_type', 'is_weekend', 'order_time_of_day']\n",
    "        self.ordinal_cat_cols = ['traffic', 'distance_type']\n",
    "        self.target_col = 'time_taken'\n",
    "        self.traffic_order = [\"low\", \"medium\", \"high\", \"jam\"]\n",
    "        self.distance_type_order = [\"short\", \"medium\", \"long\", \"very_long\"]\n",
    "\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"scale\", MinMaxScaler(), self.num_cols),\n",
    "                (\"nominal_encode\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False), self.nominal_cat_cols),\n",
    "                (\"ordinal_encode\", OrdinalEncoder(categories=[self.traffic_order, self.distance_type_order],\n",
    "                                                  encoded_missing_value=-999, handle_unknown=\"use_encoded_value\",\n",
    "                                                  unknown_value=-1), self.ordinal_cat_cols)\n",
    "            ],\n",
    "            remainder=\"passthrough\",\n",
    "            n_jobs=-1,\n",
    "            verbose_feature_names_out=False\n",
    "        )\n",
    "\n",
    "    def load_data(self, data_path: Path) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_csv(data_path)\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            logger.error(\"The file to load does not exist\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def drop_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(f\"The original dataset has {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "        df_dropped = data.dropna()\n",
    "        logger.info(f\"The dataset after dropping missing values has {df_dropped.shape[0]} rows and {df_dropped.shape[1]} columns\")\n",
    "        return df_dropped\n",
    "\n",
    "    def make_X_and_y(self, data: pd.DataFrame):\n",
    "        X = data.drop(columns=[self.target_col])\n",
    "        y = data[self.target_col]\n",
    "        return X, y\n",
    "\n",
    "    def train_preprocessor(self, data: pd.DataFrame):\n",
    "        self.preprocessor.fit(data)\n",
    "        return self.preprocessor\n",
    "\n",
    "    def perform_transformations(self, data: pd.DataFrame):\n",
    "        return self.preprocessor.transform(data)\n",
    "\n",
    "    def join_X_and_y(self, X: pd.DataFrame, y: pd.Series):\n",
    "        return X.join(y, how='inner')\n",
    "\n",
    "    def save_data(self, data: pd.DataFrame, save_path: Path):\n",
    "        data.to_csv(save_path, index=False)\n",
    "\n",
    "    def save_transformer(self):\n",
    "        joblib.dump(self.preprocessor, self.transformer_save_dir / \"preprocessor.joblib\")\n",
    "\n",
    "    def run_transformation_pipeline(self):\n",
    "        train_df = self.drop_missing_values(self.load_data(self.train_data_path))\n",
    "        logger.info(\"Train data loaded successfully\")\n",
    "        test_df = self.drop_missing_values(self.load_data(self.test_data_path))\n",
    "        logger.info(\"Test data loaded successfully\")\n",
    "        \n",
    "        X_train, y_train = self.make_X_and_y(train_df)\n",
    "        X_test, y_test = self.make_X_and_y(test_df)\n",
    "        logger.info(\"Data splitting completed\")\n",
    "\n",
    "        self.train_preprocessor(X_train)\n",
    "        logger.info(\"Preprocessor is trained\")\n",
    "\n",
    "        X_train_trans = self.perform_transformations(X_train)\n",
    "        logger.info(\"Train data is transformed\")\n",
    "        X_test_trans = self.perform_transformations(X_test)\n",
    "        logger.info(\"Test data is transformed\")\n",
    "\n",
    "        train_trans_df = self.join_X_and_y(pd.DataFrame(X_train_trans), y_train)\n",
    "        test_trans_df = self.join_X_and_y(pd.DataFrame(X_test_trans), y_test)\n",
    "        logger.info(\"Datasets joined\")\n",
    "\n",
    "        self.save_data(train_trans_df, self.save_train_trans_path)\n",
    "        self.save_data(test_trans_df, self.save_test_trans_path)\n",
    "        logger.info(\"Transformed data saved\")\n",
    "\n",
    "        self.save_transformer()\n",
    "        logger.info(\"Preprocessor saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-11 17:57:42,532: INFO: common: yaml file: E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\config\\config.yaml loaded successfully]\n",
      "[2025-02-11 17:57:42,535: INFO: common: yaml file: E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\params.yaml loaded successfully]\n",
      "[2025-02-11 17:57:42,540: INFO: common: yaml file: E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\schema.yaml loaded successfully]\n",
      "[2025-02-11 17:57:42,543: INFO: common: created directory at: artifacts]\n",
      "[2025-02-11 17:57:42,547: INFO: common: created directory at: artifacts/data_trans/]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_data_trans_config()\n",
    "    data_validation = DataTransformation(config=data_validation_config)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-11 17:57:44,986: INFO: 77540092: The original dataset has 28271 rows and 16 columns]\n",
      "[2025-02-11 17:57:45,013: INFO: 77540092: The dataset after dropping missing values has 28271 rows and 16 columns]\n",
      "[2025-02-11 17:57:45,017: INFO: 77540092: Train data loaded successfully]\n",
      "[2025-02-11 17:57:45,083: INFO: 77540092: The original dataset has 9424 rows and 16 columns]\n",
      "[2025-02-11 17:57:45,096: INFO: 77540092: The dataset after dropping missing values has 9424 rows and 16 columns]\n",
      "[2025-02-11 17:57:45,098: INFO: 77540092: Test data loaded successfully]\n",
      "[2025-02-11 17:57:45,106: INFO: 77540092: Data splitting completed]\n",
      "[2025-02-11 17:57:45,285: INFO: 77540092: Preprocessor is trained]\n",
      "[2025-02-11 17:57:45,422: INFO: 77540092: Train data is transformed]\n",
      "[2025-02-11 17:57:45,488: INFO: 77540092: Test data is transformed]\n",
      "[2025-02-11 17:57:45,504: INFO: 77540092: Datasets joined]\n",
      "[2025-02-11 17:57:46,676: INFO: 77540092: Transformed data saved]\n",
      "[2025-02-11 17:57:46,686: INFO: 77540092: Preprocessor saved]\n"
     ]
    }
   ],
   "source": [
    "data_validation.run_transformation_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataTransformation:\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        self.config = config  # ✅ Fix: Use self.config\n",
    "\n",
    "    num_cols = [\"age\", \"ratings\", \"pickup_time_minutes\", \"distance\"]\n",
    "    nominal_cat_cols = ['weather', 'type_of_order', 'type_of_vehicle', 'festival', 'city_type', 'is_weekend', 'order_time_of_day']\n",
    "    ordinal_cat_cols = ['traffic', 'distance_type']\n",
    "    target_col = 'time_taken'\n",
    "\n",
    "    # Order for ordinal encoding\n",
    "    traffic_order = [\"low\", \"medium\", \"high\", \"jam\"]\n",
    "    distance_type_order = [\"short\", \"medium\", \"long\", \"very_long\"]\n",
    "\n",
    "    def load_data(self, data_path: Path) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_csv(data_path)  # ✅ Fix: Pass file path\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"The file {data_path} does not exist\")\n",
    "            return pd.DataFrame()  # ✅ Fix: Return an empty DataFrame\n",
    "\n",
    "    def drop_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(f\"Original dataset: {data.shape[0]} rows, {data.shape[1]} columns\")\n",
    "        df_dropped = data.dropna()\n",
    "        logger.info(f\"After dropping missing values: {df_dropped.shape[0]} rows, {df_dropped.shape[1]} columns\")\n",
    "        \n",
    "        if df_dropped.isna().sum().sum() > 0:\n",
    "            raise ValueError(\"The dataframe still has missing values!\")\n",
    "        \n",
    "        return df_dropped\n",
    "\n",
    "    def run_transformation_pipeline(self):\n",
    "        \"\"\" Load, clean, and preprocess data \"\"\"\n",
    "        input_path = Path(self.config.data_input_dir)  # ✅ Fix: Use self.config\n",
    "        data_path = input_path / \"train.csv\"\n",
    "\n",
    "        # Load data\n",
    "        train_data = self.load_data(data_path)\n",
    "        print(train_data.shape)\n",
    "\n",
    "        # Drop missing values\n",
    "        train_df = self.drop_missing_values(data=train_data)\n",
    "\n",
    "\n",
    "# ✅ Usage\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
