{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\projects\\\\Delivery-time-prediction-for-food-devlivery-industry\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\projects\\\\Delivery-time-prediction-for-food-devlivery-industry'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_input_dir:Path\n",
    "    model_path: Path\n",
    "    metric_file: Path\n",
    "    all_params: dict\n",
    "    prepro_dir: Path\n",
    "    #metric_file_name: Path\n",
    "    #target_column: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\config\\config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\params.yaml\")\n",
    "SCHEMA_FILE_PATH = Path(\"E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\schema.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deliveryprediction.constants import *\n",
    "from Deliveryprediction.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.LightGBM\n",
    "        #schema =  self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_input_dir=config.data_input_dir,\n",
    "            model_path = config.model_path,\n",
    "            prepro_dir = config.prepro_dir,\n",
    "            all_params=params,\n",
    "            metric_file = config.metric_file,\n",
    "            #target_column = schema.name\n",
    "           \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from Deliveryprediction import logger\n",
    "import mlflow\n",
    "import dagshub\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import json\n",
    "\n",
    "class ModelEvaluation:\n",
    "    \n",
    "    TARGET_COLUMN = \"time_taken\"\n",
    "    def __init__(self,logger, repo_owner, repo_name, experiment_name, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.target_column = \"time_taken\"\n",
    "        self.root_dir = Path(self.config.root_dir)\n",
    "        self.root_path = Path(self.config.data_input_dir)\n",
    "        self.prepro_dir = Path(self.config.prepro_dir)\n",
    "        self.train_data_path = self.root_path / \"train_trans.csv\"\n",
    "        self.test_data_path = self.root_path / \"test_trans.csv\"\n",
    "        self.model_path = Path(self.config.model_path)\n",
    "        self.metric_path = Path(self.config.metric_file)\n",
    "\n",
    "        # self.save_data_dir = Path(self.config.root_dir)\n",
    "        # self.save_data_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # self.model_save_dir = self.save_data_dir / \"models\"\n",
    "        # self.model_save_dir.mkdir(exist_ok=True)\n",
    "        # self.training_data = None\n",
    "        # self.model = None\n",
    "        # self.stacking_model = None\n",
    "        # self.transformer = None\n",
    "         \n",
    "        # Initialize Dagshub and MLflow\n",
    "        dagshub.init(repo_owner=repo_owner, repo_name=repo_name, mlflow=True)\n",
    "        mlflow.set_tracking_uri(f\"https://dagshub.com/{repo_owner}/{repo_name}.mlflow\")\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "       \n",
    "    def load_data(self, data_path: Path) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_csv(data_path)\n",
    "            self.logger.info(f\"Data loaded successfully from {data_path}\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"File not found: {data_path}\")\n",
    "            return None\n",
    "    \n",
    "    def split_data(self, data: pd.DataFrame):\n",
    "        X = data.drop(columns=[self.target_column])\n",
    "        y = data[self.target_column]\n",
    "        return X, y\n",
    "    \n",
    "    def load_model(self, model_path: Path):\n",
    "        try:\n",
    "            model = joblib.load(model_path)\n",
    "            self.logger.info(\"Model loaded successfully\")\n",
    "            return model\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"Model file not found: {model_path}\")\n",
    "            return None\n",
    "    \n",
    "    def save_model_info(self, metric_path: Path, run_id, artifact_path, model_name):\n",
    "        info_dict = {\"run_id\": run_id, \"artifact_path\": artifact_path, \"model_name\": model_name}\n",
    "        with open(metric_path, \"w\") as f:\n",
    "            json.dump(info_dict, f, indent=4)\n",
    "        self.logger.info(\"Model information saved\")\n",
    "    \n",
    "    def evaluate_model(self, model, X_train, y_train, X_test, y_test):\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "        mean_cv_score = -cv_scores.mean()\n",
    "        \n",
    "        self.logger.info(\"Model evaluation completed\")\n",
    "        return train_mae, test_mae, train_r2, test_r2, mean_cv_score, cv_scores\n",
    "    \n",
    "    def log_metrics_to_mlflow(self, model, train_mae, test_mae, train_r2, test_r2, mean_cv_score, cv_scores, X_train, train_data, test_data, root_path):\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.set_tag(\"model\", \"Food Delivery Time Regressor\")\n",
    "            mlflow.log_params(model.get_params())\n",
    "            mlflow.log_metric(\"train_mae\", train_mae)\n",
    "            mlflow.log_metric(\"test_mae\", test_mae)\n",
    "            mlflow.log_metric(\"train_r2\", train_r2)\n",
    "            mlflow.log_metric(\"test_r2\", test_r2)\n",
    "            mlflow.log_metric(\"mean_cv_score\", mean_cv_score)\n",
    "            mlflow.log_metrics({f\"CV {num}\": -score for num, score in enumerate(cv_scores)})\n",
    "            \n",
    "            train_data_input = mlflow.data.from_pandas(train_data, targets=self.target_column)\n",
    "            test_data_input = mlflow.data.from_pandas(test_data, targets=self.target_column)\n",
    "            mlflow.log_input(dataset=train_data_input, context=\"training\")\n",
    "            mlflow.log_input(dataset=test_data_input, context=\"validation\")\n",
    "            \n",
    "            model_signature = mlflow.models.infer_signature(X_train.sample(20, random_state=42), model.predict(X_train.sample(20, random_state=42)))\n",
    "            mlflow.sklearn.log_model(model, \"delivery_time_pred_model\", signature=model_signature)\n",
    "            \n",
    "            mlflow.log_artifact(self.root_dir / \"models\" / \"stacking_regressor.joblib\")\n",
    "            mlflow.log_artifact(self.root_dir / \"models\" / \"power_transformer.joblib\")\n",
    "            mlflow.log_artifact(self.prepro_dir)\n",
    "            \n",
    "            artifact_uri = mlflow.get_artifact_uri()\n",
    "            self.logger.info(\"MLflow logging complete\")\n",
    "            return run.info.run_id, artifact_uri\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        train_data = self.load_data(self.train_data_path)\n",
    "        test_data = self.load_data(self.test_data_path)\n",
    "        X_train, y_train = self.split_data(train_data)\n",
    "        X_test, y_test = self.split_data(test_data)\n",
    "        \n",
    "        model = self.load_model(self.model_path)\n",
    "        train_mae, test_mae, train_r2, test_r2, mean_cv_score, cv_scores = self.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        run_id, artifact_uri = self.log_metrics_to_mlflow(model, train_mae, test_mae, train_r2, test_r2, mean_cv_score, cv_scores, X_train, train_data, test_data, self.root_path)\n",
    "        \n",
    "        save_json_path = self.metric_path\n",
    "        self.save_model_info(save_json_path, run_id, artifact_uri, \"delivery_time_pred_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 16:54:16,572: INFO: common: yaml file: E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\config\\config.yaml loaded successfully]\n",
      "[2025-02-18 16:54:16,578: INFO: common: yaml file: E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\params.yaml loaded successfully]\n",
      "[2025-02-18 16:54:16,586: INFO: common: yaml file: E:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\schema.yaml loaded successfully]\n",
      "[2025-02-18 16:54:16,589: INFO: common: created directory at: artifacts]\n",
      "[2025-02-18 16:54:16,591: INFO: common: created directory at: artifacts/model_trainer/]\n",
      "[2025-02-18 16:54:17,621: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/repos/onkar-git/Delivery-time-prediction-for-food-delivery-industry \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"onkar-git/Delivery-time-prediction-for-food-delivery-industry\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"onkar-git/Delivery-time-prediction-for-food-delivery-industry\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 16:54:17,629: INFO: helpers: Initialized MLflow to track repo \"onkar-git/Delivery-time-prediction-for-food-delivery-industry\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository onkar-git/Delivery-time-prediction-for-food-delivery-industry initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository onkar-git/Delivery-time-prediction-for-food-delivery-industry initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 16:54:17,635: INFO: helpers: Repository onkar-git/Delivery-time-prediction-for-food-delivery-industry initialized!]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "model_config = config.get_model_evaluation_config()\n",
    "model_final = ModelEvaluation(logger=logger,experiment_name= 'delivery_prediction_experiemnt_1', repo_owner='onkar-git', repo_name='Delivery-time-prediction-for-food-delivery-industry', config=model_config)\n",
    "\n",
    "# model_gbm=ModelTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 16:54:19,698: INFO: 1188005277: Data loaded successfully from artifacts\\data_trans\\train_trans.csv]\n",
      "[2025-02-18 16:54:19,868: INFO: 1188005277: Data loaded successfully from artifacts\\data_trans\\test_trans.csv]\n",
      "[2025-02-18 16:54:20,947: INFO: 1188005277: Model loaded successfully]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 479 out of 479 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 479 out of 479 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 16:56:03,887: INFO: 1188005277: Model evaluation completed]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\.venv\\lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 479 out of 479 | elapsed:    0.1s finished\n",
      "e:\\projects\\Delivery-time-prediction-for-food-devlivery-industry\\.venv\\lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 16:57:45,249: INFO: 1188005277: MLflow logging complete]\n",
      "🏃 View run adventurous-stag-295 at: https://dagshub.com/onkar-git/Delivery-time-prediction-for-food-delivery-industry.mlflow/#/experiments/0/runs/2e0b2791d67d42a0ab0a5ed536fc3bda\n",
      "🧪 View experiment at: https://dagshub.com/onkar-git/Delivery-time-prediction-for-food-delivery-industry.mlflow/#/experiments/0\n",
      "[2025-02-18 16:57:45,940: INFO: 1188005277: Model information saved]\n"
     ]
    }
   ],
   "source": [
    "model_final.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
